name: Load S3 to Supabase (Resumable)

on:
  workflow_dispatch:
    inputs:
      source:
        description: "Source to load (source_id, all, or auto for next pending)"
        required: true
        default: "auto"
      dry_run:
        description: "Dry run (count only, no inserts)"
        type: boolean
        default: false
      vacuum:
        description: "Run VACUUM ANALYZE after load (heavier)"
        type: boolean
        default: false

  schedule:
    - cron: "0 2 * * *" # daily 2am UTC
    - cron: "15 * * * *" # hourly catch-up for overnight backfill

jobs:
  load:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    concurrency:
      group: s3-to-supabase-resumable
      cancel-in-progress: false

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - run: pip install psycopg2-binary boto3

      - name: Load from S3 to Supabase
        env:
          POSTGRES_HOST: ${{ secrets.POSTGRES_HOST }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}

          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_BUCKET_NAME: ${{ secrets.AWS_BUCKET_NAME }}

          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
          DO_VACUUM: ${{ github.event.inputs.vacuum || 'false' }}
          CHUNK_SIZE: "10000"
          MAX_RETRIES: "3"
          RETRY_BASE_SECONDS: "2"
          DB_STATEMENT_TIMEOUT_MS: "0"
          DB_CONNECT_RETRIES: "12"
          DB_CONNECT_RETRY_SECONDS: "5"
        run: |
          python s3_to_supabase_one_by_one.py --source "${{ github.event.inputs.source || 'auto' }}"
