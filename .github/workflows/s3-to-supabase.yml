name: Load S3 to Supabase (Resumable)

on:
  workflow_dispatch:
    inputs:
      source:
        description: "Source to load (york_buildings, york_addresses, or all)"
        required: true
        default: "all"
      dry_run:
        description: "Dry run (count only, no inserts)"
        type: boolean
        default: false
      vacuum:
        description: "Run VACUUM ANALYZE after load (heavier)"
        type: boolean
        default: false

  schedule:
    - cron: "0 2 * * *" # daily 2am UTC
    - cron: "15 * * * *" # hourly catch-up for overnight backfill

jobs:
  load:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    concurrency:
      group: s3-to-supabase-resumable
      cancel-in-progress: false

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - run: pip install psycopg2-binary boto3

      - name: Load from S3 to Supabase
        env:
          POSTGRES_HOST: ${{ secrets.POSTGRES_HOST }}
          POSTGRES_DB: ${{ secrets.POSTGRES_DB }}
          POSTGRES_USER: ${{ secrets.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_PORT: ${{ secrets.POSTGRES_PORT }}

          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_BUCKET_NAME: ${{ secrets.AWS_BUCKET_NAME }}

          SOURCE_FILTER: ${{ github.event.inputs.source || 'all' }}
          DRY_RUN: ${{ github.event.inputs.dry_run }}
          DO_VACUUM: ${{ github.event.inputs.vacuum }}
          BATCH_SIZE: "10000"
          CHUNK_SIZE: "10000"
          MAX_RETRIES: "3"
          RETRY_BASE_SECONDS: "2"
          DB_STATEMENT_TIMEOUT_MS: "0"
          DB_CONNECT_RETRIES: "12"
          DB_CONNECT_RETRY_SECONDS: "5"

        run: |
          python << 'PY'
          import os, json, tempfile, logging, re, socket, time
          import boto3, psycopg2
          from psycopg2.extras import execute_values

          logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
          log = logging.getLogger("loader")

          # Force IPv4 for direct DB hosts only.
          # Keep Supabase pooler hostnames intact (SNI/routing can break on raw IP).
          _host = os.environ.get("POSTGRES_HOST", "").strip()
          if _host:
            if _host.endswith("pooler.supabase.com"):
              log.info("Using pooler hostname without IPv4 rewrite: %s", _host)
            else:
              try:
                infos = socket.getaddrinfo(_host, None, socket.AF_INET)
                if infos:
                  ipv4 = infos[0][4][0]
                  os.environ["POSTGRES_HOST"] = ipv4
                  log.info("Resolved POSTGRES_HOST to IPv4: %s", ipv4)
                else:
                  log.warning("No IPv4 for %r; connection may fail (use Supabase pooler host if so)", _host)
              except Exception as e:
                log.warning("Could not resolve to IPv4: %s", e)

          BUCKET = os.environ["AWS_BUCKET_NAME"]
          FILTER = os.environ.get("SOURCE_FILTER", "all")
          DRY_RUN = str(os.environ.get("DRY_RUN", "false")).lower() == "true"
          DO_VACUUM = str(os.environ.get("DO_VACUUM", "false")).lower() == "true"
          BATCH = int(os.environ.get("BATCH_SIZE", "10000"))
          CHUNK_SIZE = int(os.environ.get("CHUNK_SIZE", "10000"))
          MAX_RETRIES = int(os.environ.get("MAX_RETRIES", "3"))
          RETRY_BASE_SECONDS = int(os.environ.get("RETRY_BASE_SECONDS", "2"))
          DB_STATEMENT_TIMEOUT_MS = int(os.environ.get("DB_STATEMENT_TIMEOUT_MS", "0"))
          DB_CONNECT_RETRIES = int(os.environ.get("DB_CONNECT_RETRIES", "12"))
          DB_CONNECT_RETRY_SECONDS = int(os.environ.get("DB_CONNECT_RETRY_SECONDS", "5"))

          s3 = boto3.client("s3")

          def pg_conn():
            last_error = None
            for attempt in range(1, DB_CONNECT_RETRIES + 1):
              try:
                conn = psycopg2.connect(
                  host=os.environ["POSTGRES_HOST"],
                  database=os.environ["POSTGRES_DB"],
                  user=os.environ["POSTGRES_USER"],
                  password=os.environ["POSTGRES_PASSWORD"],
                  port=os.environ["POSTGRES_PORT"],
                  connect_timeout=30,
                  keepalives=1,
                  keepalives_idle=30,
                  keepalives_interval=10,
                  keepalives_count=5,
                )
                with conn.cursor() as cur:
                  cur.execute("SET statement_timeout = %s", (DB_STATEMENT_TIMEOUT_MS,))
                return conn
              except psycopg2.OperationalError as e:
                last_error = e
                msg = str(e).lower()
                retryable = any(token in msg for token in [
                  "maxclientsinsessionmode",
                  "max clients",
                  "too many connections",
                  "timeout",
                  "timed out",
                  "connection",
                ])
                if not retryable or attempt >= DB_CONNECT_RETRIES:
                  raise
                sleep_s = DB_CONNECT_RETRY_SECONDS * attempt
                log.warning(
                  "DB connect attempt %s/%s failed (%s). Retrying in %ss...",
                  attempt,
                  DB_CONNECT_RETRIES,
                  str(e),
                  sleep_s,
                )
                time.sleep(sleep_s)
            if last_error:
              raise last_error
            raise psycopg2.OperationalError("Failed to establish DB connection")

          def is_retryable_error(err):
            msg = str(err).lower()
            if isinstance(err, (psycopg2.OperationalError, psycopg2.InterfaceError)):
              return True
            retry_tokens = [
              "timeout",
              "timed out",
              "connection",
              "server closed",
              "connection reset",
              "broken pipe",
              "could not connect",
              "ssl",
            ]
            return any(token in msg for token in retry_tokens)

          def ensure_loaded_files_table():
            if DRY_RUN:
              return
            conn = pg_conn()
            try:
              with conn.cursor() as cur:
                cur.execute("""
                  CREATE TABLE IF NOT EXISTS public.loader_loaded_files (
                    s3_key text PRIMARY KEY,
                    source_id text NOT NULL,
                    status text NOT NULL DEFAULT 'in_progress',
                    rows_loaded bigint NOT NULL DEFAULT 0,
                    attempts integer NOT NULL DEFAULT 0,
                    last_error text,
                    updated_at timestamptz NOT NULL DEFAULT now(),
                    loaded_at timestamptz
                  );
                """)
              conn.commit()
            finally:
              conn.close()

          def is_file_completed(s3_key):
            if DRY_RUN:
              return False
            conn = pg_conn()
            try:
              with conn.cursor() as cur:
                cur.execute(
                  "SELECT 1 FROM public.loader_loaded_files WHERE s3_key = %s AND status = 'completed'",
                  (s3_key,),
                )
                return cur.fetchone() is not None
            finally:
              conn.close()

          def mark_file_status(s3_key, source_id, status, rows_loaded=0, last_error=None):
            if DRY_RUN:
              return
            conn = pg_conn()
            try:
              with conn.cursor() as cur:
                cur.execute("""
                  INSERT INTO public.loader_loaded_files (
                    s3_key, source_id, status, rows_loaded, attempts, last_error, updated_at, loaded_at
                  ) VALUES (
                    %s,
                    %s,
                    %s,
                    %s,
                    CASE WHEN %s = 'in_progress' THEN 1 ELSE 0 END,
                    %s,
                    now(),
                    CASE WHEN %s = 'completed' THEN now() ELSE NULL END
                  )
                  ON CONFLICT (s3_key) DO UPDATE SET
                    source_id = EXCLUDED.source_id,
                    status = EXCLUDED.status,
                    rows_loaded = EXCLUDED.rows_loaded,
                    attempts = CASE
                      WHEN EXCLUDED.status = 'in_progress' THEN public.loader_loaded_files.attempts + 1
                      ELSE public.loader_loaded_files.attempts
                    END,
                    last_error = EXCLUDED.last_error,
                    updated_at = now(),
                    loaded_at = CASE WHEN EXCLUDED.status = 'completed' THEN now() ELSE public.loader_loaded_files.loaded_at END;
                """, (s3_key, source_id, status, rows_loaded, status, last_error, status))
              conn.commit()
            finally:
              conn.close()

          def run_with_retries(load_fn, source_id, key):
            attempts = 0
            while attempts < MAX_RETRIES:
              attempts += 1
              try:
                mark_file_status(key, source_id, "in_progress")
                rows = load_fn(source_id, key)
                mark_file_status(key, source_id, "completed", rows_loaded=rows or 0)
                return rows
              except Exception as e:
                err_msg = str(e)[:2000]
                mark_file_status(key, source_id, "failed", last_error=err_msg)
                retryable = is_retryable_error(e)
                if not retryable or attempts >= MAX_RETRIES:
                  raise
                sleep_s = RETRY_BASE_SECONDS * (2 ** (attempts - 1))
                log.warning(
                  "Retrying %s after attempt %s/%s failed: %s (sleep %ss)",
                  key,
                  attempts,
                  MAX_RETRIES,
                  err_msg,
                  sleep_s,
                )
                time.sleep(sleep_s)

          def stream_records(key):
            with tempfile.NamedTemporaryFile(delete=False) as tmp:
              s3.download_file(BUCKET, key, tmp.name)
              tmp_path = tmp.name
            try:
              with open(tmp_path, "r") as f:
                for line in f:
                  if line.strip():
                    yield json.loads(line)
            finally:
              os.unlink(tmp_path)

          def list_all_files(prefix="gold-standard/canada/ontario/"):
            files = []
            paginator = s3.get_paginator("list_objects_v2")
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
              for obj in page.get("Contents", []):
                k = obj["Key"]
                if k.endswith("_gold.ndjson"):
                  parts = k.split("/")
                  # gold-standard/canada/ontario/<source_id>/<yyyymmdd>/<source_id>_gold.ndjson
                  if len(parts) >= 5:
                    source_id = parts[-3]
                    date_folder = parts[-2]
                    files.append((source_id, date_folder, k, obj["Size"]))
            return files

          def pick_latest_per_source(files):
            latest = {}
            for source_id, date_folder, key, size in files:
              if source_id not in latest or date_folder > latest[source_id][0]:
                latest[source_id] = (date_folder, key, size)
            return [(sid, v[0], v[1], v[2]) for sid, v in latest.items()]

          def filter_sources(files):
            if FILTER == "all":
              return files
            return [f for f in files if FILTER in f[0] or FILTER in f[2]]

          def load_buildings(source_id, key):
            if DRY_RUN:
              c = 0
              for r in stream_records(key):
                if r.get("external_id") is not None:
                  c += 1
              log.info(f"[DRY RUN] buildings {source_id}: {c:,} (non-null external_id)")
              return c

            conn = pg_conn()
            try:
              with conn.cursor() as cur:
                cur.execute("""
                  CREATE TEMP TABLE _stg_buildings (
                    source_id text,
                    source_file text,
                    source_url text,
                    source_date date,
                    external_id text,
                    parcel_id text,
                    geom text,
                    centroid text,
                    area_sqm double precision,
                    height_m double precision,
                    floors integer,
                    year_built integer,
                    building_type text,
                    subtype text,
                    primary_address text,
                    primary_street_number text,
                    primary_street_name text
                  ) ON COMMIT DROP;
                """)

              batch, total = [], 0
              for r in stream_records(key):
                if r.get("external_id") is None:
                  continue

                batch.append((
                  r.get("source_id"), r.get("source_file"), r.get("source_url"), r.get("source_date"),
                  r.get("external_id"), r.get("parcel_id"),
                  r.get("geom"), r.get("centroid"),
                  r.get("area_sqm"), r.get("height_m"), r.get("floors"), r.get("year_built"),
                  r.get("building_type"), r.get("subtype"),
                  r.get("primary_address"), r.get("primary_street_number"), r.get("primary_street_name")
                ))

                if len(batch) >= BATCH:
                  with conn.cursor() as cur:
                    execute_values(cur, "INSERT INTO _stg_buildings VALUES %s", batch)
                  total += len(batch)
                  log.info(f"  staged buildings: {total:,}")
                  batch = []

              if batch:
                with conn.cursor() as cur:
                  execute_values(cur, "INSERT INTO _stg_buildings VALUES %s", batch)
                total += len(batch)

              with conn.cursor() as cur:
                cur.execute("""
                  INSERT INTO public.ref_buildings_gold (
                    source_id, source_file, source_url, source_date,
                    external_id, parcel_id, geom, centroid, area_sqm,
                    height_m, floors, year_built, building_type, subtype,
                    primary_address, primary_street_number, primary_street_name
                  )
                  SELECT
                    source_id, source_file, source_url, source_date,
                    external_id, parcel_id,
                    ST_Multi(ST_GeomFromEWKT(geom)),
                    CASE WHEN centroid IS NULL THEN NULL ELSE ST_GeomFromEWKT(centroid) END,
                    area_sqm, height_m, floors, year_built, building_type, subtype,
                    primary_address, primary_street_number, primary_street_name
                  FROM _stg_buildings
                  ON CONFLICT ON CONSTRAINT uniq_ref_buildings_source_external
                  DO UPDATE SET
                    geom = EXCLUDED.geom,
                    centroid = EXCLUDED.centroid,
                    area_sqm = EXCLUDED.area_sqm,
                    height_m = EXCLUDED.height_m,
                    floors = EXCLUDED.floors,
                    year_built = EXCLUDED.year_built,
                    building_type = EXCLUDED.building_type,
                    subtype = EXCLUDED.subtype,
                    updated_at = now();
                """)

              conn.commit()
              log.info(f"✅ loaded buildings {source_id}: {total:,}")
              return total
            finally:
              conn.close()

          def norm_addr(s):
            # Match DB normalization: collapse whitespace so "123  Main" and "123 Main" dedupe to one
            return " ".join((s or "").lower().split())

          def load_addresses(source_id, key):
            if DRY_RUN:
              c = sum(1 for _ in stream_records(key))
              log.info(f"[DRY RUN] addresses {source_id}: {c:,}")
              return c

            conn = pg_conn()
            try:
              with conn.cursor() as cur:
                cur.execute("""
                  CREATE TEMP TABLE _stg_addresses (
                    source_id text,
                    source_file text,
                    source_url text,
                    source_date date,
                    street_number text,
                    street_name text,
                    unit text,
                    city text,
                    zip text,
                    province text,
                    country text,
                    geom text,
                    address_type text,
                    precision text
                  ) ON COMMIT DROP;
                """)

              # Dedupe in Python so one row per (source_id, norm street_number, norm street_name, city, unit); keep latest source_date
              seen = {}
              for r in stream_records(key):
                k = (r.get("source_id"), norm_addr(r.get("street_number")), norm_addr(r.get("street_name")), (r.get("city") or ""), (r.get("unit") or ""))
                rec = (r.get("source_id"), r.get("source_file"), r.get("source_url"), r.get("source_date"), r.get("street_number"), r.get("street_name"), r.get("unit"), r.get("city"), r.get("zip"), r.get("province", "ON"), r.get("country", "CA"), r.get("geom"), r.get("address_type"), r.get("precision", "rooftop"))
                sd = r.get("source_date") or ""
                if k not in seen or (sd and str(sd) > str(seen[k][0])):
                  seen[k] = (sd, rec)
              rows = [v[1] for v in seen.values()]
              total = len(rows)
              log.info(f"  addresses deduped: {total:,} unique")

              for i in range(0, total, BATCH):
                batch = rows[i:i + BATCH]
                with conn.cursor() as cur:
                  execute_values(cur, "INSERT INTO _stg_addresses VALUES %s", batch)
                log.info(f"  staged addresses: {min(i + BATCH, total):,}")

              with conn.cursor() as cur:
                cur.execute("""
                  CREATE TEMP TABLE _chunk (
                    source_id text, source_file text, source_url text, source_date date,
                    street_number text, street_name text, unit text, city text, zip text,
                    province text, country text, geom text, address_type text, precision text
                  ) ON COMMIT DROP;
                """)
              with conn.cursor() as cur:
                cur.execute("""
                  CREATE TEMP TABLE _chunk_norm (
                    LIKE public.ref_addresses_gold INCLUDING DEFAULTS INCLUDING GENERATED
                  ) ON COMMIT DROP;
                """)

              inserted_total = 0
              while True:
                with conn.cursor() as cur:
                  cur.execute("""
                    WITH d AS (
                      DELETE FROM _stg_addresses
                      WHERE ctid IN (SELECT ctid FROM _stg_addresses LIMIT %s)
                      RETURNING *
                    )
                    INSERT INTO _chunk SELECT * FROM d
                  """, (CHUNK_SIZE,))
                  moved = cur.rowcount
                if moved == 0:
                  break
                with conn.cursor() as cur:
                  cur.execute("""
                    INSERT INTO _chunk_norm (
                      source_id, source_file, source_url, source_date,
                      street_number, street_name, unit, city, zip, province, country,
                      geom, address_type, precision
                    )
                    SELECT
                      source_id, source_file, source_url, source_date,
                      street_number, street_name, unit, city, zip, province, country,
                      CASE
                        WHEN geom IS NULL OR btrim(geom) = '' THEN NULL
                        ELSE ST_SetSRID(
                          ST_PointOnSurface(ST_GeomFromEWKT(geom)),
                          4326
                        )::geometry(Point,4326)
                      END,
                      address_type, precision
                    FROM _chunk;
                  """)
                with conn.cursor() as cur:
                  cur.execute("""
                    INSERT INTO public.ref_addresses_gold (
                      source_id, source_file, source_url, source_date,
                      street_number, street_name, unit, city, zip, province, country,
                      geom, address_type, precision
                    )
                    SELECT
                      source_id, source_file, source_url, source_date,
                      street_number, street_name, unit, city, zip, province, country,
                      geom, address_type, precision
                    FROM (
                      SELECT DISTINCT ON (
                        source_id, street_number_normalized, street_name_normalized, city, unit
                      )
                        source_id, source_file, source_url, source_date,
                        street_number, street_name, unit, city, zip, province, country,
                        geom, address_type, precision
                      FROM _chunk_norm
                      ORDER BY
                        source_id,
                        street_number_normalized,
                        street_name_normalized,
                        city,
                        unit,
                        source_date DESC NULLS LAST
                    ) sub
                    ON CONFLICT ON CONSTRAINT uniq_ref_addr_source_norm_city_unit
                    DO UPDATE SET
                      zip = EXCLUDED.zip,
                      geom = EXCLUDED.geom,
                      address_type = EXCLUDED.address_type,
                      precision = EXCLUDED.precision,
                      updated_at = now();
                  """)
                  n_inserted = cur.rowcount
                  inserted_total += n_inserted
                with conn.cursor() as cur:
                  cur.execute("TRUNCATE _chunk")
                with conn.cursor() as cur:
                  cur.execute("TRUNCATE _chunk_norm")
                log.info(f"  address chunk: {moved:,} staged -> {n_inserted:,} inserted (running total: {inserted_total:,})")
              log.info(f"  address chunks done: {inserted_total:,} rows into ref_addresses_gold")

              conn.commit()
              log.info(f"✅ loaded addresses {source_id}: {total:,}")
              return total
            finally:
              conn.close()

          # MAIN
          ensure_loaded_files_table()
          all_files = list_all_files()
          all_files = filter_sources(all_files)

          latest = pick_latest_per_source(all_files)
          log.info(f"Found {len(latest)} latest file(s) to process")

          # Load addresses first, then buildings (helps if you later choose linking)
          for source_id, date_folder, key, size in sorted(latest, key=lambda x: x[2]):
            if "address" in key.lower():
              if is_file_completed(key):
                log.info("Skipping already completed file: %s", key)
                continue
              run_with_retries(load_addresses, source_id, key)

          for source_id, date_folder, key, size in sorted(latest, key=lambda x: x[2]):
            if "building" in key.lower():
              if is_file_completed(key):
                log.info("Skipping already completed file: %s", key)
                continue
              run_with_retries(load_buildings, source_id, key)

          # ANALYZE always; VACUUM optional
          if not DRY_RUN:
            conn = pg_conn()
            conn.autocommit = True
            with conn.cursor() as cur:
              log.info("ANALYZE ref_buildings_gold...")
              cur.execute("ANALYZE public.ref_buildings_gold")
              log.info("ANALYZE ref_addresses_gold...")
              cur.execute("ANALYZE public.ref_addresses_gold")
              if DO_VACUUM:
                log.info("VACUUM ANALYZE ref_buildings_gold...")
                cur.execute("VACUUM ANALYZE public.ref_buildings_gold")
                log.info("VACUUM ANALYZE ref_addresses_gold...")
                cur.execute("VACUUM ANALYZE public.ref_addresses_gold")
            conn.close()

          log.info("✅ done")
          PY
